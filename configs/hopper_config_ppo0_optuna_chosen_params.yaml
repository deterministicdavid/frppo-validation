

# --- General Env ---
env_name: "Hopper-v5"
env_is_atari: false
env_is_mujoco: true
n_stack: 0

# --- Training Parameters ---
train:
  algo: "PPO"
  run_id: "optuna_chosen_0" # for tensorboard logging
  policy: "default" # either "own" or "default"
  n_envs: 1 # Recommended for Mujoco
  n_steps: 2048 # Following the 37 implementation details of Proximal Policy Optimization
  batch_size: 512 # They say batch size nminibatches = 32 so 1 x 2048 / 32 = 64
  n_opt_epochs: 10 
  total_timesteps: 400000 # we want to see if we replicate optuna values and this was what number of timesteps they took
  ent_coef: 0.00 # Again following the 37 implementation details of Proximal Policy Optimization for mujoco
  decay_lr_final_frac: 0.9986556159077401
  decay_lr_init: 0.002697808561575481
  clip_epsilon: 0.18818432207835933
  force_cpu: false

# --- Logging & Saving ---
logging:
  log_dir: "./logs_hopper/"
  name_prefix: "ppo_hopper_optuna_chosen_vals_latest"
  checkpoint_save_freq: 10000 # OverwriteCheckpointCallback save frequency

# --- Visualization ---
visualize:
  video_folder: "videos_hopper/"
  deterministic: true
